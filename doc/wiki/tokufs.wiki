TokuFS is a virtual, user-space file system optimized for microdata
updates without fragmentation. TokuFS is imlpemented as a library,
libtokufs, which exports a small, posix-like interface. It's 
functionality includes:

 mount(local_path) \\attempt to mount tokufs at the given local path
 unmount() \\unmount tokufs
 open(&toku_fd, tokufs_path, pid) \\open a file in the TokuFS namespace as a certain process 
                                    and data read or written will only correspond to that pid.
 close(toku_fd) \\close an open file
 read_at(toku_fd, buf, count, offset) \\read at a given offset
 write_at(toku_fd, buf, count, offset)\\ write at a given offset

Because TokuFS is a virtual file system, it requires an existing 
underlying file system for storage. The mount and unmount functions
allow an application to specify where on the local file system TokuFS 
should write its data. In this way, TokuFS is a layered file system, 
where the top layer is a TokuFS namespace and fractal tree index, and 
the bottom layer is a local namespace on some traditional file system. 

TokuFS builds on top of TokuDB, an implementation of key value
storage using fractal trees. TokuFS views files as a collection of
blocks, each numbered incrementally from 0. These block numbers are
used as keys into TokuDB, where the value is a block of bytes to
hold the data. Minus the database, this is exactly how real file
systems view files. 

For simplicity, there exists a single database for each file. An 
optimized approach would use a dedicated set of databases to hold
metadata, and a dedicated set for data blocks. The keys into the
metadata database would be TokuFS filenames, and the values would
be some information about which data block database to open to get
the data for that particular file. Additionally, the data block databases
would use a file indentifier plus block number for its keys, to
disambiguate similar block numbers for different files. 

In addition to the TokuFS API described above, applications can
interface with TokuFS using MPI IO through an interface called ADIO.
The ADIO interface allows storage systems such as TokuFS to interpose
on parallel MPI IO applications transparently. All these applications
need to do is refer to files with the prefix 'tokufs:', and libtokufs
is used to read and write the file data.

MPI IO is used to coordinate the IO among parallel processes for
high performance. Since TokuDB environments and dictionaries are only 
accessible to one process at a time, the ADIO implementation represents
files as the union of each processes share in the underlying TokuFS file.
The 'share' of a certain pid P for file F is defined as all of the data
written to file F by processes previously opening the file as pid P.
Multiple processes writing to the same logical region of file F 
(and thus different shares) produces undefined behavior, so the code 
assumes this does not happen. When a process wants to open the share 
for pid P of file F, the open call looks like:
    open(&toku_fd, F, P);
Subsequent reads will only produce data if the region read was previously
written to by some process that opened file F as pid P. In the simplest
case, the reading process is the same as the writing process.

To help illustrate how files are represented in the ADIO layer for
TokuFS, consider 4 processes writing non overlapping 4 byte segments to
form a 16 byte file. The following psuedo code is run by each process 
in parallel:
    
    // who are we in the world of mpi? get a unique process identifer,
    // which is somewhere in the range of 0..N-1 for N processes
    pid = get_mpi_io_pid();
    // open out.file as that pid, then write 10 bytes of stuff.
    tokufs_open(&fd, "myfile", pid);
    // write out our pid to the file, which is simply 4 bytes.
    tokufs_write_at(fd, &pid, 4, pid * 4)
    tokufs_close(fd);

TokuFS uses a container directory to house the TokuDB environment used
by each process taking part in the write of some file. At the top level,
the TokuFS mount point looks like:

                            /mnt/tokufs
                                .
                                .
          env.0         env.1       env.2         env.3
            .             .           .             .
            .             .           .             .
     data.tokudb    data.tokudb    data.tokudb     data.tokudb

The contents of each data.tokudb form sparse files whose union yields
the logical ADIO file. Graphically:

    env.0/data.tokudb
        ['0', null, null, null]
    env.1/data.tokudb
        [null, '1', null null]
    env.2/data.tokudb
        [null, null, '2', null]
    env.3/data.tokudb
        [null, null, null, '3']
    = ADIO logical "/mnt/tokufs/myfile"
        ['0', '1', '2', '3']

The current implementation limits each process to read only what it
wrote into its particular share while each process has its share open.
This was to assist in rapid development and debugging, since N-1
checkpointing applications only read the data they write anyway. It is
very important to note that the ADIO design does prevent this from
happening, just the current implementation. One could imagine a scheme
where MPI processes use a kind of RPC to request data from each other
at the ADIO layer.
